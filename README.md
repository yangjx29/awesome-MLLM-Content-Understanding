# awesome-MLLM-Content-Understanding

<h1 align="center"> Awesome MLLM Content Understanding</h1>

<div align="center">

[LICENSE](https://img.shields.io/github/license/Xnhyacinth/Awesome-LLM-Long-Context-Modeling)

![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)

[commit](https://img.shields.io/github/last-commit/yangjx29/awesome-MLLM-Content-Understanding?color=blue)

[PR](https://img.shields.io/badge/PRs-Welcome-red)

[GitHub Repo stars](https://img.shields.io/github/stars/yangjx29/awesome-MLLM-Content-Understanding)

<!--

[license](https://img.shields.io/bower/l/bootstrap?style=plastic)

-->

</div>

A curated collection of papers on **Multimodal Content Understanding (MCU)**,  
including **Fine-Grained Visual Recognition/Classification (FGVR/FGVC)**,  
**Content Moderation with Collaborative Large and Small Models**, etc.  

We believe that multimodal-based content understanding is still a largely unexplored field,  
and we hope this repository will provide you with some valuable insights!  

---

### Contents:

- [FGVR/FGVC](#1-fgvrfgvc)

- [Content Moderation with Collaborative Large and Small Models](#2-content-moderation-with-collaborative-large-and-small-models)

---

### 1. FGVR/FGVC

### 2024
1. **[Democratizing Fine-grained Visual Recognition with Large Language Models](https://arxiv.org/abs/2401.13837)**  
   *ICLR 2024* [[code](https://github.com/OatmealLiu/FineR)]  
   TL;DR: `Proposes a training-free pipeline leveraging LLM reasoning to democratize fine-grained visual recognition in open-vocabulary settings`


---


### 2. Content Moderation with Collaborative Large and Small Models