<h1 align="center"> Awesome MLLM Content Understanding</h1>

<div align="center">

[![LICENSE](https://img.shields.io/github/license/yangjx29/awesome-MLLM-Content-Understanding)](https://github.com/yangjx29/awesome-MLLM-Content-Understanding/blob/master/LICENSE)
![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)
[![commit](https://img.shields.io/github/last-commit/yangjx29/awesome-MLLM-Content-Understanding?color=blue)](https://github.com/yangjx29/awesome-MLLM-Content-Understanding/commits/master)
[![PR](https://img.shields.io/badge/PRs-Welcome-red)](https://github.com/yangjx29/awesome-MLLM-Content-Understanding/pulls)
[![GitHub Repo stars](https://img.shields.io/github/stars/yangjx29/awesome-MLLM-Content-Understanding)](https://github.com/yangjx29/awesome-MLLM-Content-Understanding)

</div>

A curated collection of papers on **Multimodal Content Understanding (MCU)**,  
including **Fine-Grained Visual Recognition/Classification (FGVR/FGVC)**,  
**Content Moderation with Collaborative Large and Small Models**, etc.  

We believe that multimodal-based content understanding is still a largely unexplored field,  and we hope this repository will provide you with some valuable insights!  

---

<!-- ### 2024
1. **[paper](url)**  
   *Meeting Name If Any* [[code]()]  
   TL;DR: `` -->

### Contents:

- [FGVR/FGVC](#1-fgvrfgvc)

- [Content Moderation with Collaborative Large and Small Models](#2-content-moderation-with-collaborative-large-and-small-models)

---

### 1. FGVR/FGVC

### 2025 
1. **[Vocabulary-free Fine-grained Visual Recognition via Enriched Contextually Grounded Vision-Language Model](https://www.arxiv.org/abs/2507.23070)**  
   *ICCV 2025* [[code](https://github.com/demidovd98/e-finer)]  
   TL;DR: `Training-free, Vocabulary-free,Resource-constrained Settings`

1. <u>[**DyFo: A Training-Free Dynamic Focus Visual Search for Enhancing LMMs in Fine-Grained Visual Understanding**](https://arxiv.org/abs/2504.14920)</u>

   *CVPR* 2025[[<u>code</u>](https://github.com/PKU-ICST-MIPL/DyFo_CVPR2025)]

   TL;DR: `Training-free,visual search,using a Monte Carlo Tree Search (MCTS) algorithm to simulate human-like focus adjustments`

1. **[Unlabeled Data Improves Fine-Grained Image Zero-shot Classification with Multimodal LLMs](https://arxiv.org/abs/2506.03195)**

   *arxiv 2025*[[code](https://github.com/yq-hong/AutoSEP)]

   TL;DR:`Prompt learning，Zero-shot image Classification,Self-supervised optimization framework`

1. **[Dynamic Multimodal Prototype Learning in Vision-Language Models](https://arxiv.org/abs/2507.03657)**

   *ICCV 2025*

   TL;DR:`Dynamic Multimodal Prototypes,Test-Time Adaptation,Visual Particles Updating,Optimal Transport`

1. **[UniFGVC: Universal Training-Free Few-Shot Fine-Grained Vision Classification via Attribute-Aware Multimodal Retrieval](https://arxiv.org/abs/2508.04136)**

   *arxiv 2025*

   TL;DR:`Training-Free Multimodal Retrieval,Category-Discriminative Visual Captioner,Reference-Guided Chain-of-Thought,Similarity-Based Retrieval`

1. [**Enhancing Zero-Shot Image Recognition in Vision-Language  Models through Human-like Concept Guidance**](https://arxiv.org/abs/2503.15886)

   *arxiv 2025*

   TL;DR:`Concept-guided Human-like Bayesian Reasoning,Importance Sampling,Leveraging priors,Bayes’ Theorem`

### 2024

1. **[Democratizing Fine-grained Visual Recognition with Large Language Models](https://arxiv.org/abs/2401.13837)**  
   *ICLR 2024* [[code](https://github.com/OatmealLiu/FineR)]  
   TL;DR: ` Proposes a training-free pipeline leveraging LLM reasoning to democratize fine-grained visual recognition in open-vocabulary settings`

2. **[Label Propagation for Zero-shot Classification with Vision-Language Models](https://arxiv.org/abs/2404.04072)**

   *CVPR 2024*[[code](https://github.com/vladan-stojnic/ZLaP)]

   TL;DR: `Zero-shot Classification,Label propagation,leverage the graph structure of the unlabeled data`

3. [**AWT: Transferring Vision-Language Models via Augmentation, Weighting, and Transportation**](https://arxiv.org/abs/2407.04603)

   *NeurIPS 2024*[[code](https://github.com/MCG-NJU/AWT)]

   TL;DR:`Visual-Text Mutual Augmentation,Entropy Weighting,Optimal Transport`

4. [**Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs**](https://arxiv.org/abs/2403.11755)

   *ECCV 2024*[[code](https://github.com/jmiemirza/Meta-Prompting)]

   TL;DR:`Two-Stage Prompting,Automated Prompt Generations,Prompt Ensembling`

5. **[Retrieval-enriched zero-shot image classification in low-resource domains](https://arxiv.org/abs/2411.00988)**

   *EMNLP 2024*[[code](https://github.com/Fodark/CoRE)]

   TL;DR:`Retrieval-Enriched Representation,Low-Resource Domains,Textual Information Retrieval,Contextual Prototype Enrichment`

6. [**Enhancing Fine-Grained Image Classifications via Cascaded Vision Language Models**](https://arxiv.org/abs/2405.11301)

   *EMNLP  finding 2024*

   TL;DR:`CLIP–LVLM Integration,Candidate Class Ranking,Few-Shot Classification,Contextual Prompt Construction,Adaptive Entropy Threshold`

7. [**Hypergraph-guided Intra- and Inter-category Relation Modeling for Fine-grained Visual Recognition**](https://dl.acm.org/doi/abs/10.1145/3664647.3680589)

   *arxiv 2024*

   TL;DR:`Intra-category Distinctiveness，Inter-category Similarity,Hypergraph-guided Intra- and Inter-category Relation Modeling,Hypergraph-guided Structure Learning,Inter-category Relation Perception`

### 2023

1. **[Intra-Modal Proxy Learning for Zero-Shot Visual Categorization with CLIP](https://arxiv.org/abs/2310.19752)**

   *NeurIPS 2023*

   TL;DR:`Zeroshot visual categorizations,Intra-modal proxy learning,Modality Gap,Pseudo-Label Refinement`
   
1. **[SuS-X: Training-Free Name-Only Transfer of Vision-Language Models](https://arxiv.org/abs/2211.16198)**

   *ICCV 2023*[[code](https://github.com/vishaal27/SuS-X)]

   TL;DR:`Training-Free Name-Only Transfer,Few-Shot Learning,Zero-Shot Classification,TIP-X Module`


---


### 2. Content Moderation with Collaborative Large and Small Models
1. 