<h1 align="center"> Awesome MLLM Content Understanding</h1>

<div align="center">

[![LICENSE](https://img.shields.io/github/license/yangjx29/awesome-MLLM-Content-Understanding)](https://github.com/yangjx29/awesome-MLLM-Content-Understanding/blob/master/LICENSE)
![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)
[![commit](https://img.shields.io/github/last-commit/yangjx29/awesome-MLLM-Content-Understanding?color=blue)](https://github.com/yangjx29/awesome-MLLM-Content-Understanding/commits/master)
[![PR](https://img.shields.io/badge/PRs-Welcome-red)](https://github.com/yangjx29/awesome-MLLM-Content-Understanding/pulls)
[![GitHub Repo stars](https://img.shields.io/github/stars/yangjx29/awesome-MLLM-Content-Understanding)](https://github.com/yangjx29/awesome-MLLM-Content-Understanding)

</div>

A curated collection of papers on **Multimodal Content Understanding (MCU)**,  
including **Fine-Grained Visual Recognition/Classification (FGVR/FGVC)**,  
**Content Moderation with Collaborative Large and Small Models**, etc.  

We believe that multimodal-based content understanding is still a largely unexplored field,  and we hope this repository will provide you with some valuable insights!  

---

<!-- ### 2024
1. **[paper](url)**  
   *Meeting Name If Any* [[code]()]  
   TL;DR: `` -->

### Contents:

- [FGVR/FGVC](#1-fgvrfgvc)

- [Content Moderation with Collaborative Large and Small Models](#2-content-moderation-with-collaborative-large-and-small-models)

---

### 1. FGVR/FGVC

### 2025 
1. **[Vocabulary-free Fine-grained Visual Recognition via Enriched Contextually Grounded Vision-Language Model](https://www.arxiv.org/abs/2507.23070)**  
   *ICCV 2025* [[code](https://github.com/demidovd98/e-finer)]  
   TL;DR: `Training-free, Vocabulary-free,Resource-constrained Settings`
   
1. <u>[**DyFo: A Training-Free Dynamic Focus Visual Search for Enhancing LMMs in Fine-Grained Visual Understanding**](https://arxiv.org/abs/2504.14920)</u>
   
   *CVPR* 2025[[<u>code</u>](https://github.com/PKU-ICST-MIPL/DyFo_CVPR2025)]
   
   TL;DR: `Training-free,visual search,using a Monte Carlo Tree Search (MCTS) algorithm to simulate human-like focus adjustments`

### 2024

1. **[Democratizing Fine-grained Visual Recognition with Large Language Models](https://arxiv.org/abs/2401.13837)**  
   *ICLR 2024* [[code](https://github.com/OatmealLiu/FineR)]  
   TL;DR: `Proposes a training-free pipeline leveraging LLM reasoning to democratize fine-grained visual recognition in open-vocabulary settings`

2. **[Label Propagation for Zero-shot Classification with Vision-Language Models](https://arxiv.org/abs/2404.04072)**

   *CVPR* 2024[[code](https://github.com/vladan-stojnic/ZLaP)]

   TL;DR: `Zero-shot Classification,label propagation,leverage the graph structure of the unlabeled data`

### 2023

1. **[Intra-Modal Proxy Learning for Zero-Shot Visual Categorization with CLIP](https://proceedings.neurips.cc/paper_files/paper/2023/hash/50a057e9fe79ffa3f4120fb6fb88071a-Abstract-Conference.html)**

   *NeurIPS* 2023

   TL;DR:`zeroshot visual categorizations,intra-modal proxy learning`


---


### 2. Content Moderation with Collaborative Large and Small Models
1. 